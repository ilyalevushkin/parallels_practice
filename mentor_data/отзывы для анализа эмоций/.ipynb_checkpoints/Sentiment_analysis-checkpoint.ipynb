{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.probability  import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_pos_tag(words):\n",
    "    lower_words = []\n",
    "    for i in words:\n",
    "        lower_words.append(i.lower())\n",
    "    pos_words = pos_tag(lower_words)\n",
    "    return pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(words):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_words = []\n",
    "    types = ['JJ', 'JJR', 'JJS', \n",
    "             'MD', 'NN', 'NNS', \n",
    "             'NP', 'NPS', 'POS', \n",
    "             'PP', 'RB', 'RBR', \n",
    "             'RBS', 'VV', 'VVD', \n",
    "             'VVG', 'VVN', 'VVP', 'VVZ']\n",
    "    for i in words:\n",
    "        if i[1] in types:\n",
    "            cleaned_words.append(wordnet_lemmatizer.lemmatize(i[0]))\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_by_status(data_path, status):\n",
    "    frame = pd.read_csv(data_path, header=0, sep=',')\n",
    "    return frame[frame['status'].isin([status])].review.to_list()\n",
    "\n",
    "def get_review_by_label(data_path, label):\n",
    "    if (label == 'neutral'):\n",
    "        return get_review_by_status(data_path, 0)\n",
    "    elif (label == 'bad'):\n",
    "        return get_review_by_status(data_path, -1)\n",
    "    elif (label == 'good'):\n",
    "        return get_review_by_status(data_path, 1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process(data_path, label):\n",
    "    # Wordmatrix - список документов с лексемами\n",
    "    # All words - список всех слов\n",
    "    data = {'Word_matrix': [], 'All_words': []}\n",
    "    # Промежуточный список для удаления гапаксов\n",
    "    templist_allwords = []\n",
    "    reviews = get_review_by_label(data_path, label)\n",
    "    # Создание токенайзера\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
    "    for review in reviews: # Обработка корпуса\n",
    "        bag_words = tokenizer.tokenize(review)\n",
    "        lower_words = lower_pos_tag(bag_words)\n",
    "        cleaned_words = clean(lower_words)\n",
    "        finalist = list(bigrams(cleaned_words)) + cleaned_words\n",
    "        data['Word_matrix'].append(finalist)\n",
    "        templist_allwords.extend(cleaned_words)\n",
    "    # Определение гапаксов\n",
    "    templistfreq = FreqDist(templist_allwords)\n",
    "    hapaxes = templistfreq.hapaxes()\n",
    "    # Фильтрация от гапаксов\n",
    "    for word in templist_allwords:\n",
    "        if word not in hapaxes:\n",
    "            data['All_words'].append(word)\n",
    "    return {label: data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "labels = ['neutral', 'bad', 'good']\n",
    "data.update(process('updated_dataset.csv', labels[0]))\n",
    "data.update(process('updated_dataset.csv', labels[1]))\n",
    "data.update(process('updated_dataset.csv', labels[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание помеченный данных со структурой:\n",
    "# [([список слов отзыва], метка_класса)]\n",
    "def union_data(data):\n",
    "    labels = ['neutral', 'bad', 'good']\n",
    "    labeled_data = []\n",
    "    for label in labels:\n",
    "        for document in data[label]['Word_matrix']:\n",
    "            labeled_data.append((document, label))\n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabular_create(labels, data):\n",
    "    # Создание вокабуляра с уникальными лексемами\n",
    "    all_words = [] \n",
    "    for label in labels:\n",
    "        frequency = FreqDist(data[label]['All_words'])\n",
    "        common_words = frequency.most_common(1000)\n",
    "        words = [i[0] for i in common_words]\n",
    "        all_words.extend(words)\n",
    "    # Извлечение уникальных лексем\n",
    "    unique_words = list(OrderedDict.fromkeys(all_words))\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    " def frequency_coding(labeled_data, unique_words):\n",
    "    # Частотное кодирование для классификаторов scikit-learn\n",
    "    # Разреженная матрица для признаков\n",
    "    matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray()\n",
    "    # Массив для меток классов\n",
    "    target = np.zeros(len(labeled_data), 'str')\n",
    "    for index_doc, document in enumerate(labeled_data):\n",
    "        for index_word, word in enumerate(unique_words):\n",
    "            # Подсчет кол-ва вхождения слова в отзыв\n",
    "            matrix_vec[index_doc, index_word] = document[0].count(word)\n",
    "        target[index_doc] = document[1]\n",
    "    return matrix_vec, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Считывание, обработка и векторизация обучающих данных\n",
    "teaching_data = {}\n",
    "labels = ['neutral', 'bad', 'good']\n",
    "for label in labels:\n",
    "    teaching_data.update(process('appstore_parallels_modified.csv', label))\n",
    "\n",
    "unique_words = vocabular_create(labels, teaching_data)\n",
    "\n",
    "matrix_vec, target = frequency_coding(union_data(teaching_data), unique_words)\n",
    "\n",
    "# Перемешиваем датасет\n",
    "X, Y = shuffle(matrix_vec, target)\n",
    "\n",
    "model = MultinomialNB(0.1)\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка на отзывах Parallels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительная обработка теста\n",
    "testing_data = {}\n",
    "labels = ['neutral', 'bad', 'good']\n",
    "for label in labels:\n",
    "    testing_data.update(process('TrustPilot_modified.csv', label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация теста\n",
    "X_test, Y_test = frequency_coding(union_data(testing_data), unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7813620071684588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           b       0.10      1.00      0.18         6\n",
      "           g       1.00      0.78      0.88       272\n",
      "           n       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.78       279\n",
      "   macro avg       0.37      0.59      0.35       279\n",
      "weighted avg       0.98      0.78      0.86       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "# Точность на контрольном датасете\n",
    "score_test = accuracy_score(Y_test, predicted)\n",
    "# Классификационный отчет\n",
    "report = classification_report(Y_test, predicted)\n",
    "\n",
    "print(score_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка на отзывах Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительная обработка теста\n",
    "testing_data = {}\n",
    "labels = ['neutral', 'bad', 'good']\n",
    "for label in labels:\n",
    "    testing_data.update(process('appstore_excel_modified.csv', label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация теста\n",
    "X_test, Y_test = frequency_coding(union_data(testing_data), unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49333333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           b       0.46      0.82      0.59       100\n",
      "           g       0.54      0.60      0.57       100\n",
      "           n       0.50      0.06      0.11       100\n",
      "\n",
      "    accuracy                           0.49       300\n",
      "   macro avg       0.50      0.49      0.42       300\n",
      "weighted avg       0.50      0.49      0.42       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "# Точность на контрольном датасете\n",
    "score_test = accuracy_score(Y_test, predicted)\n",
    "# Классификационный отчет\n",
    "report = classification_report(Y_test, predicted)\n",
    "\n",
    "print(score_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
